{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression From Scratch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(point1, point2):\n",
    "    distance = 0\n",
    "    for i in range(len(point1)):\n",
    "        distance += (point1[i] - point2[i]) ** 2\n",
    "    return distance ** 0.5\n",
    "\n",
    "def knn_predict(test_point, data, labels, k):\n",
    "    distances = []\n",
    "    for i in range(len(data)):\n",
    "        distances.append((euclidean_distance(test_point, data[i]), labels[i]))\n",
    "    \n",
    "    distances.sort()\n",
    "    \n",
    "    neighbors = distances[:k]\n",
    "    \n",
    "    votes = [0, 0]\n",
    "    for distance, label in neighbors:\n",
    "        votes[label] += 1\n",
    "    \n",
    "    return 1 if votes[1] > votes[0] else 0\n",
    "\n",
    "def calculate_accuracy(test_data, test_labels, train_data, train_labels, k):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        predicted_label = knn_predict(test_data[i], train_data, train_labels, k)\n",
    "        if predicted_label == test_labels[i]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(test_data)\n",
    "    return accuracy\n",
    "\n",
    "def calculate_performance(test_data, test_labels, train_data, train_labels, k):\n",
    "    predictions = [knn_predict(point, train_data, train_labels, k) for point in test_data]\n",
    "    tn, fp, fn, tp = confusion_matrix(test_labels, predictions).ravel()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    auc = roc_auc_score(test_labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": (tp + tn) / (tp + tn + fp + fn),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"auc\": auc\n",
    "    }\n",
    "\n",
    "def evaluate_metrics(true_labels, predictions):\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "\n",
    "def bias_variance_analysis(train_data, train_labels, test_data, test_labels, k_values):\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        train_predictions = [knn_predict(point, train_data, train_labels, k) for point in train_data]\n",
    "        test_predictions = [knn_predict(point, train_data, train_labels, k) for point in test_data]\n",
    "\n",
    "        train_metrics = evaluate_metrics(train_labels, train_predictions)\n",
    "        test_metrics = evaluate_metrics(test_labels, test_predictions)\n",
    "\n",
    "        results.append({\n",
    "            \"k\": k,\n",
    "            \"train_performance\": train_metrics,\n",
    "            \"test_performance\": test_metrics\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def perform_kfold_cross_validation(data, labels, k, n_splits=10):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    performances = []\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        trainingData, testingData = [data[i] for i in train_index], [data[i] for i in test_index]\n",
    "        trainingLabels, testingLabels = [labels[i] for i in train_index], [labels[i] for i in test_index]\n",
    "\n",
    "        # Calculate performance for this fold\n",
    "        performance = calculate_performance(testingData, testingLabels, trainingData, trainingLabels, k)\n",
    "        performances.append(performance)\n",
    "\n",
    "    # Aggregate and return the performances\n",
    "    return {\n",
    "        metric: sum([perf[metric] for perf in performances]) / n_splits\n",
    "        for metric in performances[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read in the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line into tokens based on whitespace separation\n",
    "            tokens = line.strip().split()\n",
    "\n",
    "            # Convert \"Present\" or \"Absent\" to binary. Assuming this is always the 5th column in the data.\n",
    "            binary_feature = 1 if tokens[4] == \"Present\" else 0\n",
    "            \n",
    "            # Replace the \"Present\"/\"Absent\" with its binary representation\n",
    "            tokens[4] = binary_feature\n",
    "\n",
    "            # All tokens except the last are features (convert all to floats)\n",
    "            features = [float(token) for token in tokens[:-1]]\n",
    "\n",
    "            # The last token is the label (convert to int)\n",
    "            label = int(tokens[-1])\n",
    "\n",
    "            # Add the feature vector and label to the data lists\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "dataset1 = './project3_dataset1.txt'\n",
    "dataset2 = './project3_dataset2.txt'\n",
    "\n",
    "data1, labels1 = load_data(dataset1)\n",
    "data2, labels2 = load_data(dataset2)\n",
    "\n",
    "# Let's Split our Data into a training and testing set\n",
    "training_testing_boundary = .7\n",
    "data1Split = int(len(data1) * training_testing_boundary)\n",
    "data2Split = int(len(data2) * training_testing_boundary)\n",
    "\n",
    "\n",
    "trainingData1 = data1[:data1Split]\n",
    "trainingData2 = data2[:data2Split]\n",
    "trainingLabels1 = labels1[:data1Split]\n",
    "trainingLabels2 = labels2[:data2Split]\n",
    "\n",
    "testingData1 = data1[data1Split:]\n",
    "testingData2 = data2[data2Split:]\n",
    "testingLabels1 = labels1[data1Split:]\n",
    "testingLabels2 = labels2[data2Split:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Training Performance: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}\n",
      "Testing Performance: {'accuracy': 0.9122807017543859, 'precision': 0.8840579710144928, 'recall': 0.8970588235294118, 'f1_score': 0.8905109489051095}\n",
      "k = 3\n",
      "Training Performance: {'accuracy': 0.9547738693467337, 'precision': 0.9565217391304348, 'recall': 0.9166666666666666, 'f1_score': 0.9361702127659574}\n",
      "Testing Performance: {'accuracy': 0.9181286549707602, 'precision': 0.8970588235294118, 'recall': 0.8970588235294118, 'f1_score': 0.8970588235294118}\n",
      "k = 5\n",
      "Training Performance: {'accuracy': 0.9422110552763819, 'precision': 0.9618320610687023, 'recall': 0.875, 'f1_score': 0.9163636363636364}\n",
      "Testing Performance: {'accuracy': 0.935672514619883, 'precision': 0.9253731343283582, 'recall': 0.9117647058823529, 'f1_score': 0.9185185185185185}\n",
      "k = 10\n",
      "Training Performance: {'accuracy': 0.9422110552763819, 'precision': 0.9689922480620154, 'recall': 0.8680555555555556, 'f1_score': 0.9157509157509157}\n",
      "Testing Performance: {'accuracy': 0.9064327485380117, 'precision': 0.9193548387096774, 'recall': 0.8382352941176471, 'f1_score': 0.8769230769230769}\n",
      "k = 15\n",
      "Training Performance: {'accuracy': 0.9396984924623115, 'precision': 0.9761904761904762, 'recall': 0.8541666666666666, 'f1_score': 0.9111111111111111}\n",
      "Testing Performance: {'accuracy': 0.9122807017543859, 'precision': 0.9206349206349206, 'recall': 0.8529411764705882, 'f1_score': 0.8854961832061068}\n",
      "k = 20\n",
      "Training Performance: {'accuracy': 0.9321608040201005, 'precision': 0.975609756097561, 'recall': 0.8333333333333334, 'f1_score': 0.8988764044943819}\n",
      "Testing Performance: {'accuracy': 0.9064327485380117, 'precision': 0.9193548387096774, 'recall': 0.8382352941176471, 'f1_score': 0.8769230769230769}\n"
     ]
    }
   ],
   "source": [
    "k_values = [1, 3, 5, 10, 15, 20]  # Example values\n",
    "analysis_results = bias_variance_analysis(trainingData1, trainingLabels1, testingData1, testingLabels1, k_values)\n",
    "\n",
    "for result in analysis_results:\n",
    "    print(f\"k = {result['k']}\")\n",
    "    print(\"Training Performance:\", result['train_performance'])\n",
    "    print(\"Testing Performance:\", result['test_performance'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
